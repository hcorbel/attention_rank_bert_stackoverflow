{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyN1Pw7CipUwkE0TZ4GXwDb3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hcorbel/attention_rank_bert_stackoverflow/blob/main/Stackoverflow_attention_rank_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgYi64LVkmYs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLxI4c0dk8Bw",
        "outputId": "6aab74f3-28e3-46ef-e9a5-2e4e19cfc3ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/attention_rank/P7')"
      ],
      "metadata": {
        "id": "B30R6FTUk8IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import nltk.data"
      ],
      "metadata": {
        "id": "Jy0EYFLYuafL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1gR4yj-utY6",
        "outputId": "958248ee-9a6d-4647-ea08-939ccbf79104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "preprocessing document text to avoid breaking sentences and misunderstanding period\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "start_time = time.time()\n",
        "path_list = ['/content/gdrive/MyDrive/attention_rank/P7/stackoverflow']"
      ],
      "metadata": {
        "id": "XzEHI50Kk8Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in path_list:\n",
        "    dataset_path = item + '/'\n",
        "    text_path = dataset_path + 'docsutf8/'\n",
        "    output_path = dataset_path + 'processed_' + item + '/'\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "\n",
        "    files = os.listdir(text_path)\n",
        "    for i, file in enumerate(files):\n",
        "        files[i] = file[:-4]\n",
        "\n",
        "    files = files[:]\n",
        "    ## instead of one example files = files[:1]\n",
        "\n",
        "    for n, file in enumerate(files):\n",
        "\n",
        "        fp = open(text_path + file + '.txt', errors=\"ignore\")\n",
        "        # ai ajout√© errors=\"ignore\"\n",
        "        text = []\n",
        "        for a,line in enumerate(fp):\n",
        "            line = line.replace('\\n','')\n",
        "            if line:\n",
        "                text.append(line)\n",
        "        text = ''.join(text)\n",
        "        #print(text)\n",
        "\n",
        "        # replace or remove something for better sentences splitting\n",
        "        text = text.replace('      ', '. ').replace('     ', '. ').replace('   ', ' ')\\\n",
        "            .replace('..', '.').replace(',.', ',').replace(':.', ':').replace('?.', ':')\n",
        "        text = text.replace('Fig.', 'Figure').replace('Fig .', 'Figure')\\\n",
        "            .replace('FIG.', 'Figure').replace('FIG .', 'Figure').replace('et al.', '').replace('e.g.', '')\n",
        "\n",
        "        # split full document text into sentences\n",
        "        sentences = tokenizer.tokenize(text)\n",
        "        #print(sentences)\n",
        "\n",
        "        # double security for wrongly break a sentence into 2 pieces\n",
        "        # if the heading character of a piece is lowercase, combine it to the last piece\n",
        "        lower_lines = []\n",
        "        for l, sentence in enumerate(sentences):\n",
        "            if sentence[0].islower():\n",
        "                if sentences[l-1][-1] == '.':\n",
        "                    sentences[l - 1] = sentences[l-1][:-1]\n",
        "                sentences[l-1] += ' ' + sentence\n",
        "                lower_lines.append(l)\n",
        "        #print('empty means good', lower_lines)\n",
        "\n",
        "        final_output = []\n",
        "        for l, sentence in enumerate(sentences):\n",
        "            # if the heading character of a piece is lowercase, combine it to the last piece\n",
        "            if l not in lower_lines and any(c.isalpha() for c in sentence):  # number_only pieces are dropped, like: '1.', '3.1.'\n",
        "                final_output.append(sentence)\n",
        "        #print(final_output)\n",
        "\n",
        "        save_path = dataset_path + 'processed_docsutf8/'\n",
        "        if not os.path.exists(save_path):\n",
        "            os.mkdir(save_path)\n",
        "\n",
        "        text_file = open(save_path+file+\".txt\", \"w\")\n",
        "        text_file.write('$$$$$$'.join(final_output))\n",
        "        text_file.close()\n",
        "\n",
        "        print(n + 1, \"th file\", file, \"running time\", time.time() - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCBsywy_uoaR",
        "outputId": "d41689ef-2dbb-4d86-e187-1f6f94b76964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 th file file_12 running time 12.074652910232544\n",
            "2 th file file_84 running time 12.76214075088501\n",
            "3 th file file_19 running time 12.806665897369385\n",
            "4 th file file_97 running time 13.769248485565186\n",
            "5 th file file_96 running time 13.775821208953857\n",
            "6 th file file_25 running time 13.782207250595093\n",
            "7 th file file_58 running time 13.78989839553833\n",
            "8 th file file_67 running time 13.798341035842896\n",
            "9 th file file_43 running time 13.803743839263916\n",
            "10 th file file_81 running time 13.809549570083618\n",
            "11 th file file_89 running time 13.815261840820312\n",
            "12 th file file_28 running time 13.820624351501465\n",
            "13 th file file_56 running time 13.825842380523682\n",
            "14 th file file_24 running time 13.83076286315918\n",
            "15 th file file_47 running time 13.836674213409424\n",
            "16 th file file_8 running time 13.860742568969727\n",
            "17 th file file_38 running time 13.867972373962402\n",
            "18 th file file_87 running time 13.873560190200806\n",
            "19 th file file_85 running time 13.879451274871826\n",
            "20 th file file_61 running time 13.885680437088013\n",
            "21 th file file_35 running time 13.892253160476685\n",
            "22 th file file_42 running time 13.89809775352478\n",
            "23 th file file_95 running time 13.903793811798096\n",
            "24 th file file_37 running time 13.909154891967773\n",
            "25 th file file_20 running time 13.91466999053955\n",
            "26 th file file_92 running time 13.920728921890259\n",
            "27 th file file_55 running time 13.926298379898071\n",
            "28 th file file_62 running time 13.932164192199707\n",
            "29 th file file_10 running time 13.936815023422241\n",
            "30 th file file_99 running time 13.942534446716309\n",
            "31 th file file_60 running time 13.94776177406311\n",
            "32 th file file_33 running time 13.9531831741333\n",
            "33 th file file_51 running time 13.9580078125\n",
            "34 th file file_48 running time 13.96341347694397\n",
            "35 th file file_21 running time 13.969503402709961\n",
            "36 th file file_9 running time 13.994918823242188\n",
            "37 th file file_11 running time 14.001208066940308\n",
            "38 th file file_7 running time 14.006697416305542\n",
            "39 th file file_63 running time 14.011584520339966\n",
            "40 th file file_22 running time 14.016714811325073\n",
            "41 th file file_90 running time 14.022513389587402\n",
            "42 th file file_27 running time 14.027679204940796\n",
            "43 th file file_52 running time 14.032947063446045\n",
            "44 th file file_39 running time 14.038722038269043\n",
            "45 th file file_45 running time 14.044330596923828\n",
            "46 th file file_71 running time 14.050433158874512\n",
            "47 th file file_73 running time 14.055869102478027\n",
            "48 th file file_31 running time 14.061176538467407\n",
            "49 th file file_82 running time 14.065939664840698\n",
            "50 th file file_88 running time 14.071557521820068\n",
            "51 th file file_86 running time 14.07719612121582\n",
            "52 th file file_6 running time 14.082329511642456\n",
            "53 th file file_70 running time 14.087448596954346\n",
            "54 th file file_76 running time 14.092614889144897\n",
            "55 th file file_30 running time 14.11591124534607\n",
            "56 th file file_66 running time 14.121961116790771\n",
            "57 th file file_91 running time 14.127829313278198\n",
            "58 th file file_78 running time 14.133430480957031\n",
            "59 th file file_49 running time 14.138494968414307\n",
            "60 th file file_65 running time 14.144179582595825\n",
            "61 th file file_15 running time 14.149272918701172\n",
            "62 th file file_59 running time 14.154733180999756\n",
            "63 th file file_94 running time 14.15994668006897\n",
            "64 th file file_83 running time 14.165606260299683\n",
            "65 th file file_53 running time 14.17018437385559\n",
            "66 th file file_54 running time 14.174764394760132\n",
            "67 th file file_34 running time 14.179449558258057\n",
            "68 th file file_50 running time 14.18462324142456\n",
            "69 th file file_46 running time 14.18996548652649\n",
            "70 th file file_26 running time 14.195616483688354\n",
            "71 th file file_44 running time 14.202841758728027\n",
            "72 th file file_29 running time 14.208361625671387\n",
            "73 th file file_41 running time 14.213518381118774\n",
            "74 th file file_69 running time 14.236354351043701\n",
            "75 th file file_98 running time 14.242743015289307\n",
            "76 th file file_79 running time 14.248334169387817\n",
            "77 th file file_74 running time 14.254289388656616\n",
            "78 th file file_64 running time 14.25960397720337\n",
            "79 th file file_23 running time 14.264947414398193\n",
            "80 th file file_93 running time 14.27073860168457\n",
            "81 th file file_80 running time 14.276729345321655\n",
            "82 th file file_14 running time 14.282231092453003\n",
            "83 th file file_13 running time 14.28698992729187\n",
            "84 th file file_72 running time 14.2925443649292\n",
            "85 th file file_77 running time 14.29796028137207\n",
            "86 th file file_68 running time 14.303110122680664\n",
            "87 th file file_75 running time 14.308189868927002\n",
            "88 th file file_40 running time 14.313334465026855\n",
            "89 th file file_17 running time 14.320767402648926\n",
            "90 th file file_5 running time 14.32637643814087\n",
            "91 th file file_36 running time 14.331957340240479\n",
            "92 th file file_32 running time 14.353370428085327\n",
            "93 th file file_16 running time 14.360158681869507\n",
            "94 th file file_18 running time 14.366284608840942\n",
            "95 th file file_57 running time 14.371808528900146\n",
            "96 th file file_0 running time 15.039298295974731\n",
            "97 th file file_1 running time 16.649006128311157\n",
            "98 th file file_2 running time 17.25366997718811\n",
            "99 th file file_4 running time 17.828191995620728\n",
            "100 th file file_3 running time 18.416194438934326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import time\n",
        "from bert import tokenization"
      ],
      "metadata": {
        "id": "7CU8bbXrk8VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "split document text into paired sentences, get ready to tokenize them feed bert self-attention extractor\"\"\"\n",
        "\n",
        "\n",
        "def prep_document(document, max_sequence_length):\n",
        "    \"\"\"Does BERT-style preprocessing on the provided document.\"\"\"\n",
        "    max_num_tokens = max_sequence_length - 3\n",
        "    target_seq_length = max_num_tokens\n",
        "\n",
        "    # We DON\"T just concatenate all of the tokens from a document into a long\n",
        "    # sequence and choose an arbitrary split point because this would make the\n",
        "    # next sentence prediction task too easy. Instead, we split the input into\n",
        "    # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
        "    # input.\n",
        "    instances = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    i = 0\n",
        "    while i < len(document):\n",
        "        segment = document[i]\n",
        "        current_chunk.append(segment)\n",
        "        current_length += len(segment)\n",
        "        if i == len(document) - 1 or current_length >= target_seq_length:\n",
        "            if current_chunk:\n",
        "                a_end = 1\n",
        "                if len(current_chunk) >= 2:\n",
        "                    a_end = random.randint(1, len(current_chunk) - 1)\n",
        "\n",
        "                tokens_a = []\n",
        "                for j in range(a_end):\n",
        "                    tokens_a.extend(current_chunk[j])\n",
        "\n",
        "                tokens_b = []\n",
        "                for j in range(a_end, len(current_chunk)):\n",
        "                    tokens_b.extend(current_chunk[j])\n",
        "                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, random)\n",
        "\n",
        "                if len(tokens_a) == 0 or len(tokens_b) == 0:\n",
        "                    break\n",
        "                assert len(tokens_a) >= 1\n",
        "                assert len(tokens_b) >= 1\n",
        "\n",
        "                tokens = []\n",
        "                tokens.append(\"[CLS]\")\n",
        "                for token in tokens_a:\n",
        "                    tokens.append(token)\n",
        "                tokens.append(\"[SEP]\")\n",
        "\n",
        "                for token in tokens_b:\n",
        "                    tokens.append(token)\n",
        "                tokens.append(\"[SEP]\")\n",
        "\n",
        "                instances.append(tokens)\n",
        "\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "        i += 1\n",
        "    return instances\n",
        "\n",
        "\n",
        "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
        "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_num_tokens:\n",
        "            break\n",
        "\n",
        "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
        "        assert len(trunc_tokens) >= 1\n",
        "\n",
        "        # We want to sometimes truncate from the front and sometimes from the\n",
        "        # back to add more randomness and avoid biases.\n",
        "        if rng.random() < 0.5:\n",
        "            del trunc_tokens[0]\n",
        "        else:\n",
        "            trunc_tokens.pop()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yguqZwz_xM4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main\n",
        "dataset = '/content/gdrive/MyDrive/attention_rank/P7/stackoverflow'\n",
        "text_path = dataset + '/processed_docsutf8/'\n",
        "output_path = dataset + '/processed_stackoverflow' + '/'\n",
        "save_path = output_path + 'sentence_paired_text/'\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "bert_dir = \"/content/gdrive/MyDrive/attention_rank/P7/pretrained_bert/orgbert/\"\n",
        "\n",
        "files = os.listdir(text_path)\n",
        "for i, file in enumerate(files):\n",
        "    files[i] = file[:-4]\n",
        "\n",
        "files = files[:]\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "super_long = 0\n",
        "\n",
        "parser = argparse.ArgumentParser(description=__doc__)\n",
        "parser.add_argument(\"--num-docs\", default=1000, type=int, help=\"Number of documents to use (default=1000).\")\n",
        "parser.add_argument(\"--cased\", default=False, action='store_true', help=\"Don't lowercase the input.\")\n",
        "args, unknown = parser.parse_known_args()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=os.path.join(bert_dir, \"vocab.txt\"), do_lower_case=not args.cased)"
      ],
      "metadata": {
        "id": "DUbMx9jjxTeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n, file in enumerate(files):\n",
        "    text = ''\n",
        "    my_file = text_path + file + '.txt'\n",
        "    with open(my_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            if line:\n",
        "                text += line\n",
        "\n",
        "    file1 = open(save_path + file + \"_sentence_paired.txt\", \"a\")\n",
        "\n",
        "    for L in text.split('$$$$$$'):\n",
        "\n",
        "        sub_write = 0\n",
        "        line = tokenization.convert_to_unicode(L).strip()\n",
        "        tokens = tokenizer.tokenize(line)\n",
        "        current_doc_tokens = [tokens, tokens]\n",
        "\n",
        "        \"\"\"\n",
        "        Check if the sentence will beyond max token length or not,\n",
        "        if beyond, separate sentence to finer.\"\"\"\n",
        "\n",
        "        for segment in prep_document(current_doc_tokens, 512):\n",
        "            # if the sentence length beyond max token number, break sentence by ';' and ','\n",
        "            if (len(segment) - 1) / 2 != segment.index(\"[SEP]\"):\n",
        "                # print(segment)\n",
        "                sub_write = 1\n",
        "                super_long += 1\n",
        "                cline = L.replace(\",\", \",$$$$$\").replace(\";\", \";$$$$$\").split(\"$$$$$\")\n",
        "                for part in cline:\n",
        "                    # print(part)\n",
        "                    file1.writelines(part)\n",
        "                    file1.writelines(\"\\n\")\n",
        "                    file1.writelines(part)\n",
        "                    file1.writelines(\"\\n\")\n",
        "                    file1.writelines(\"\\n\\n\")\n",
        "        # good to write\n",
        "        if sub_write != 1:\n",
        "            file1.writelines(L)\n",
        "            file1.writelines(\"\\n\")\n",
        "            file1.writelines(L)\n",
        "            file1.writelines(\"\\n\")\n",
        "            file1.writelines(\"\\n\\n\")\n",
        "\n",
        "    file1.close()\n",
        "\n",
        "    run_time = time.time()\n",
        "    print(n, \"th file\", file, \"running time\", run_time - start_time)\n",
        "\n",
        "print('number of super long sentences:', super_long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqNFAh-RyALt",
        "outputId": "3d22e323-ce9d-4f68-f53c-e243a228bb58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 th file file_4 running time 8.473639488220215\n",
            "1 th file file_0 running time 9.074121475219727\n",
            "2 th file file_1 running time 9.819212436676025\n",
            "3 th file file_2 running time 10.388495445251465\n",
            "4 th file file_3 running time 10.95952558517456\n",
            "5 th file file_12 running time 10.969563722610474\n",
            "6 th file file_84 running time 10.978993654251099\n",
            "7 th file file_19 running time 10.988776206970215\n",
            "8 th file file_97 running time 10.994969844818115\n",
            "9 th file file_96 running time 11.000992059707642\n",
            "10 th file file_25 running time 11.008186101913452\n",
            "11 th file file_58 running time 11.016031503677368\n",
            "12 th file file_67 running time 11.02213978767395\n",
            "13 th file file_43 running time 11.030181884765625\n",
            "14 th file file_81 running time 11.057009220123291\n",
            "15 th file file_89 running time 11.065306425094604\n",
            "16 th file file_28 running time 11.074278831481934\n",
            "17 th file file_56 running time 11.08293104171753\n",
            "18 th file file_24 running time 11.08949613571167\n",
            "19 th file file_47 running time 11.096256494522095\n",
            "20 th file file_8 running time 11.103566884994507\n",
            "21 th file file_38 running time 11.110509634017944\n",
            "22 th file file_87 running time 11.119348049163818\n",
            "23 th file file_85 running time 11.126300573348999\n",
            "24 th file file_61 running time 11.133524417877197\n",
            "25 th file file_35 running time 11.141242504119873\n",
            "26 th file file_42 running time 11.149271249771118\n",
            "27 th file file_95 running time 11.156907320022583\n",
            "28 th file file_37 running time 11.167847871780396\n",
            "29 th file file_20 running time 11.174723386764526\n",
            "30 th file file_92 running time 11.183258295059204\n",
            "31 th file file_55 running time 11.190447568893433\n",
            "32 th file file_62 running time 11.212251663208008\n",
            "33 th file file_10 running time 11.219708442687988\n",
            "34 th file file_99 running time 11.226876020431519\n",
            "35 th file file_60 running time 11.233978509902954\n",
            "36 th file file_33 running time 11.240544319152832\n",
            "37 th file file_51 running time 11.246895790100098\n",
            "38 th file file_48 running time 11.253293514251709\n",
            "39 th file file_21 running time 11.265090703964233\n",
            "40 th file file_9 running time 11.272133111953735\n",
            "41 th file file_11 running time 11.280001640319824\n",
            "42 th file file_7 running time 11.286453485488892\n",
            "43 th file file_63 running time 11.292805910110474\n",
            "44 th file file_22 running time 11.30019497871399\n",
            "45 th file file_90 running time 11.310304641723633\n",
            "46 th file file_27 running time 11.316909074783325\n",
            "47 th file file_52 running time 11.322663068771362\n",
            "48 th file file_39 running time 11.329449653625488\n",
            "49 th file file_45 running time 11.33565878868103\n",
            "50 th file file_71 running time 11.343969106674194\n",
            "51 th file file_73 running time 11.365505695343018\n",
            "52 th file file_31 running time 11.379694700241089\n",
            "53 th file file_82 running time 11.386930465698242\n",
            "54 th file file_88 running time 11.394721984863281\n",
            "55 th file file_86 running time 11.401374816894531\n",
            "56 th file file_6 running time 11.411170482635498\n",
            "57 th file file_70 running time 11.41933012008667\n",
            "58 th file file_76 running time 11.428203821182251\n",
            "59 th file file_30 running time 11.436302423477173\n",
            "60 th file file_66 running time 11.44279170036316\n",
            "61 th file file_91 running time 11.450157642364502\n",
            "62 th file file_78 running time 11.456034183502197\n",
            "63 th file file_49 running time 11.465951919555664\n",
            "64 th file file_65 running time 11.474032640457153\n",
            "65 th file file_15 running time 11.481655597686768\n",
            "66 th file file_59 running time 11.488586187362671\n",
            "67 th file file_94 running time 11.496014833450317\n",
            "68 th file file_83 running time 11.504214525222778\n",
            "69 th file file_53 running time 11.512648105621338\n",
            "70 th file file_54 running time 11.533276557922363\n",
            "71 th file file_34 running time 11.541616439819336\n",
            "72 th file file_50 running time 11.548621416091919\n",
            "73 th file file_46 running time 11.558635950088501\n",
            "74 th file file_26 running time 11.56626033782959\n",
            "75 th file file_44 running time 11.574620246887207\n",
            "76 th file file_29 running time 11.585719347000122\n",
            "77 th file file_41 running time 11.593416452407837\n",
            "78 th file file_69 running time 11.59991192817688\n",
            "79 th file file_98 running time 11.609724283218384\n",
            "80 th file file_79 running time 11.61692214012146\n",
            "81 th file file_74 running time 11.624446630477905\n",
            "82 th file file_64 running time 11.631153583526611\n",
            "83 th file file_23 running time 11.639187097549438\n",
            "84 th file file_93 running time 11.652218103408813\n",
            "85 th file file_80 running time 11.660871267318726\n",
            "86 th file file_14 running time 11.671266078948975\n",
            "87 th file file_13 running time 11.677394151687622\n",
            "88 th file file_72 running time 11.685385465621948\n",
            "89 th file file_77 running time 11.708251476287842\n",
            "90 th file file_68 running time 11.716733455657959\n",
            "91 th file file_75 running time 11.724071025848389\n",
            "92 th file file_40 running time 11.730534315109253\n",
            "93 th file file_17 running time 11.744407653808594\n",
            "94 th file file_5 running time 11.75051760673523\n",
            "95 th file file_36 running time 11.757263422012329\n",
            "96 th file file_32 running time 11.763164758682251\n",
            "97 th file file_16 running time 11.771300554275513\n",
            "98 th file file_18 running time 11.778783082962036\n",
            "99 th file file_57 running time 11.785167932510376\n",
            "number of super long sentences: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Does BERT-style preprocessing of unlabeled data; heavily based on\n",
        "create_pretraining_data.py in the BERT codebase. However, does not mask words\n",
        "or ever use random paragraphs for the second text segment.\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import utils\n",
        "from bert import tokenization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOlL4-22yWHT",
        "outputId": "82a15c53-f259-4523-ef7f-dd806f0c57a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "tokenize paired sentences, get ready to feed bert self-attention extractor\"\"\"\n",
        "\n",
        "\n",
        "def prep_document(document, max_sequence_length):\n",
        "    \"\"\"Does BERT-style preprocessing on the provided document.\"\"\"\n",
        "    max_num_tokens = max_sequence_length - 3\n",
        "    target_seq_length = max_num_tokens\n",
        "\n",
        "    # We DON\"T just concatenate all of the tokens from a document into a long\n",
        "    # sequence and choose an arbitrary split point because this would make the\n",
        "    # next sentence prediction task too easy. Instead, we split the input into\n",
        "    # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
        "    # input.\n",
        "    instances = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    i = 0\n",
        "    while i < len(document):\n",
        "        segment = document[i]\n",
        "        # print(len(document), i, 'stop 2', segment)\n",
        "        current_chunk.append(segment)\n",
        "        current_length += len(segment)\n",
        "        if i == len(document) - 1 or current_length >= target_seq_length:\n",
        "            if current_chunk:\n",
        "                # print(\"cc\", current_chunk, len(current_chunk))\n",
        "                a_end = 1\n",
        "                if len(current_chunk) >= 2:\n",
        "                    a_end = random.randint(1, len(current_chunk) - 1)\n",
        "\n",
        "                tokens_a = []\n",
        "                for j in range(a_end):\n",
        "                    tokens_a.extend(current_chunk[j])\n",
        "                # print(\"aa\", tokens_a)\n",
        "\n",
        "                tokens_b = []\n",
        "                # print(\"lc\", len(current_chunk))\n",
        "                for j in range(a_end, len(current_chunk)):\n",
        "                    # print(\"j=\", j)\n",
        "                    tokens_b.extend(current_chunk[j])\n",
        "                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, random)\n",
        "                # print(\"bb\", tokens_b)\n",
        "\n",
        "                if len(tokens_a) == 0 or len(tokens_b) == 0:\n",
        "                    break\n",
        "                assert len(tokens_a) >= 1\n",
        "                assert len(tokens_b) >= 1\n",
        "\n",
        "                tokens = []\n",
        "                tokens.append(\"[CLS]\")\n",
        "                for token in tokens_a:\n",
        "                    tokens.append(token)\n",
        "                # print(\"cs\", tokens)\n",
        "                tokens.append(\"[SEP]\")\n",
        "\n",
        "                for token in tokens_b:\n",
        "                    tokens.append(token)\n",
        "                tokens.append(\"[SEP]\")\n",
        "\n",
        "                instances.append(tokens)\n",
        "\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "        i += 1\n",
        "    # print(instances)\n",
        "    return instances\n",
        "\n",
        "\n",
        "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
        "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_num_tokens:\n",
        "            break\n",
        "\n",
        "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
        "        assert len(trunc_tokens) >= 1\n",
        "\n",
        "        # We want to sometimes truncate from the front and sometimes from the\n",
        "        # back to add more randomness and avoid biases.\n",
        "        if rng.random() < 0.5:\n",
        "            del trunc_tokens[0]\n",
        "        else:\n",
        "            trunc_tokens.pop()\n",
        "\n"
      ],
      "metadata": {
        "id": "5JlJ4eL6yWSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = '/content/gdrive/MyDrive/attention_rank/P7/stackoverflow'\n",
        "text_path = dataset + '/processed_docsutf8/'\n",
        "output_path = dataset + '/processed_stackoverflow' + '/'\n",
        "save_path = output_path + 'sentence_paired_text/'\n",
        "\n",
        "# set which pre-trained bert is going to use\n",
        "bert_dir = \"/content/gdrive/MyDrive/attention_rank/P7/pretrained_bert/orgbert/\"\n",
        "bert_name = bert_dir.split('/')[1]\n",
        "\n",
        "files = os.listdir(text_path)\n",
        "for i, file in enumerate(files):\n",
        "    files[i] = file[:-4]\n",
        "\n",
        "files = files[:]\n",
        "\n",
        "start_time = time.time()\n",
        "parser = argparse.ArgumentParser(description=__doc__)\n",
        "parser.add_argument(\"--num-docs\", default=1000, type=int,help=\"Number of documents to use (default=1000).\")\n",
        "parser.add_argument(\"--cased\", default=False, action='store_true',help=\"Don't lowercase the input.\")\n",
        "parser.add_argument(\"--max_sequence_length\", default=512, type=int, help=\"Maximum input sequence length after tokenization (default=128).\")\n",
        "args, unknown = parser.parse_known_args()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=os.path.join(bert_dir, \"vocab.txt\"), do_lower_case=not args.cased)\n",
        "\n",
        "for n, file in enumerate(files):\n",
        "    data_file = save_path + file + '_sentence_paired.txt'\n",
        "    random.seed(0)\n",
        "    current_doc_tokens = []\n",
        "    segments = []\n",
        "\n",
        "    with open(data_file, \"r\") as f:\n",
        "        raw_lines = []\n",
        "        for l, line in enumerate(f):\n",
        "            raw_lines.append(line)\n",
        "            line = tokenization.convert_to_unicode(line).strip()\n",
        "            # print(line)\n",
        "            if not line:  # line is empty, deal the 2 segments in the [current doc tokens]\n",
        "                if current_doc_tokens:\n",
        "\n",
        "                    for segment in prep_document(\n",
        "                            current_doc_tokens, args.max_sequence_length):\n",
        "                        if (len(segment) - 1) / 2 != segment.index(\"[SEP]\"):\n",
        "                            # print(\"0\", segment.index(\"[SEP]\"), len(segment) - 1)\n",
        "                            # print(l, segment)\n",
        "                            cline = raw_lines[l - 1].replace(\"),\", \"),$$$$$\").split(\"$$$$$\")\n",
        "                            # for part in cline:\n",
        "                            # print(part)\n",
        "                        # print(segment)\n",
        "                        segments.append(segment)\n",
        "                        if len(segments) >= args.num_docs:\n",
        "                            break\n",
        "                    if len(segments) >= args.num_docs:\n",
        "                        break\n",
        "\n",
        "                current_doc_tokens = []  # clean up [current doc tokens]\n",
        "\n",
        "            tokens = tokenizer.tokenize(line)\n",
        "            if tokens:  # if line is not empty, add lines to [current doc tokens]\n",
        "                current_doc_tokens.append(tokens)\n",
        "    run_time = time.time()\n",
        "    print(n, \"th file\", file, \"running time\", run_time - start_time)\n",
        "    utils.write_json([{\"tokens\": s} for s in segments],\n",
        "                     data_file.replace(\"_sentence_paired.txt\", \"\")+\"_\"+bert_name+\".json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIg1VuPXyWbp",
        "outputId": "8e1f9959-a542-4523-d113-33fe3a2f0039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 th file file_4 running time 0.08666753768920898\n",
            "1 th file file_0 running time 0.8966028690338135\n",
            "2 th file file_1 running time 1.497527837753296\n",
            "3 th file file_2 running time 2.0931649208068848\n",
            "4 th file file_3 running time 2.6513452529907227\n",
            "5 th file file_12 running time 3.2333762645721436\n",
            "6 th file file_84 running time 3.2437708377838135\n",
            "7 th file file_19 running time 3.2592251300811768\n",
            "8 th file file_97 running time 3.267986536026001\n",
            "9 th file file_96 running time 3.2749838829040527\n",
            "10 th file file_25 running time 3.283597230911255\n",
            "11 th file file_58 running time 3.292379140853882\n",
            "12 th file file_67 running time 3.299969434738159\n",
            "13 th file file_43 running time 3.309337615966797\n",
            "14 th file file_81 running time 3.31650972366333\n",
            "15 th file file_89 running time 3.325962781906128\n",
            "16 th file file_28 running time 3.337966203689575\n",
            "17 th file file_56 running time 3.3474698066711426\n",
            "18 th file file_24 running time 3.353924512863159\n",
            "19 th file file_47 running time 3.3777732849121094\n",
            "20 th file file_8 running time 3.3879048824310303\n",
            "21 th file file_38 running time 3.395355463027954\n",
            "22 th file file_87 running time 3.40354585647583\n",
            "23 th file file_85 running time 3.411311626434326\n",
            "24 th file file_61 running time 3.419644832611084\n",
            "25 th file file_35 running time 3.4277780055999756\n",
            "26 th file file_42 running time 3.437673330307007\n",
            "27 th file file_95 running time 3.4473862648010254\n",
            "28 th file file_37 running time 3.45539927482605\n",
            "29 th file file_20 running time 3.4624431133270264\n",
            "30 th file file_92 running time 3.472512722015381\n",
            "31 th file file_55 running time 3.4805448055267334\n",
            "32 th file file_62 running time 3.4898033142089844\n",
            "33 th file file_10 running time 3.4968247413635254\n",
            "34 th file file_99 running time 3.5048062801361084\n",
            "35 th file file_60 running time 3.512570858001709\n",
            "36 th file file_33 running time 3.5215256214141846\n",
            "37 th file file_51 running time 3.5439229011535645\n",
            "38 th file file_48 running time 3.5526509284973145\n",
            "39 th file file_21 running time 3.566742181777954\n",
            "40 th file file_9 running time 3.5765440464019775\n",
            "41 th file file_11 running time 3.584979772567749\n",
            "42 th file file_7 running time 3.5918028354644775\n",
            "43 th file file_63 running time 3.598546028137207\n",
            "44 th file file_22 running time 3.6078219413757324\n",
            "45 th file file_90 running time 3.6184089183807373\n",
            "46 th file file_27 running time 3.6266865730285645\n",
            "47 th file file_52 running time 3.6333680152893066\n",
            "48 th file file_39 running time 3.6423208713531494\n",
            "49 th file file_45 running time 3.650432586669922\n",
            "50 th file file_71 running time 3.660010814666748\n",
            "51 th file file_73 running time 3.668719530105591\n",
            "52 th file file_31 running time 3.679624557495117\n",
            "53 th file file_82 running time 3.6877081394195557\n",
            "54 th file file_88 running time 3.694723129272461\n",
            "55 th file file_86 running time 3.7164793014526367\n",
            "56 th file file_6 running time 3.7262823581695557\n",
            "57 th file file_70 running time 3.7346394062042236\n",
            "58 th file file_76 running time 3.7438063621520996\n",
            "59 th file file_30 running time 3.75227952003479\n",
            "60 th file file_66 running time 3.7599852085113525\n",
            "61 th file file_91 running time 3.7673182487487793\n",
            "62 th file file_78 running time 3.773669958114624\n",
            "63 th file file_49 running time 3.7841012477874756\n",
            "64 th file file_65 running time 3.791534423828125\n",
            "65 th file file_15 running time 3.7998790740966797\n",
            "66 th file file_59 running time 3.8085482120513916\n",
            "67 th file file_94 running time 3.816244602203369\n",
            "68 th file file_83 running time 3.8262386322021484\n",
            "69 th file file_53 running time 3.833944082260132\n",
            "70 th file file_54 running time 3.8402674198150635\n",
            "71 th file file_34 running time 3.8488657474517822\n",
            "72 th file file_50 running time 3.856470823287964\n",
            "73 th file file_46 running time 3.8651838302612305\n",
            "74 th file file_26 running time 3.888469934463501\n",
            "75 th file file_44 running time 3.8968193531036377\n",
            "76 th file file_29 running time 3.9057679176330566\n",
            "77 th file file_41 running time 3.9138357639312744\n",
            "78 th file file_69 running time 3.9205825328826904\n",
            "79 th file file_98 running time 3.9328691959381104\n",
            "80 th file file_79 running time 3.9424633979797363\n",
            "81 th file file_74 running time 3.949798345565796\n",
            "82 th file file_64 running time 3.956707239151001\n",
            "83 th file file_23 running time 3.964939832687378\n",
            "84 th file file_93 running time 3.9778871536254883\n",
            "85 th file file_80 running time 3.987388849258423\n",
            "86 th file file_14 running time 3.999615430831909\n",
            "87 th file file_13 running time 4.007505178451538\n",
            "88 th file file_72 running time 4.016850471496582\n",
            "89 th file file_77 running time 4.023717880249023\n",
            "90 th file file_68 running time 4.030115842819214\n",
            "91 th file file_75 running time 4.037530422210693\n",
            "92 th file file_40 running time 4.045056343078613\n",
            "93 th file file_17 running time 4.0767903327941895\n",
            "94 th file file_5 running time 4.086811542510986\n",
            "95 th file file_36 running time 4.095007658004761\n",
            "96 th file file_32 running time 4.1022117137908936\n",
            "97 th file file_16 running time 4.109304666519165\n",
            "98 th file file_18 running time 4.117605447769165\n",
            "99 th file file_57 running time 4.125911235809326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BHXHWvJ-yWk9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}